Install docker

$ sudo su -

$ yum install docker -y


Start docker

$ systemctl start docker

Scenario1:

Docker Images and pull docker image form docker hub

$ docker images
> how many images are available locally

Image name is written as : image_name:tagname

to create container, image should be available in local docker host

we need to pull image form docker hub:

$ docker pull ubuntu

$ docker pull tomcat:7

Remove Images:

$ docker rmi ubuntu

from local docker host, untagg and delete the image

$ docker rmi tomcat:7

Scenario 2:

Run docker Image to create container

$ docker run ubuntu

$ docker run nginx

press ctl c to come out of container

List all containers available on the docker host:

$ docker ps -a

ps: process(container)
-a: all process

Delete a docker container

$ docker rm -f containerid

Start the docker container in foreground mode
**************
$ docker run -it --name u1 ubuntu

run command - option:

-it : interactive terminal

> user will inside the container/ attached to the container
> container will be up & running

--name : option to give unique name to container
docker host: every container should have unique name


Come out of attached container and keep the container running:

press ctl p + ctl q



Only container create using the "mode" can be started, stopped & attached to it

$ docker run -it --name u1 ubuntu

$ ctlp+ctlq

$ docker stop u1

$ docker start u1

$ docker attach u1  ===> user will be on the terminal of the conatiner


start the container in detached mode/background mode
************

$ docker run -d --name n1 nginx

> Container will be up & running
> user cursor will be on the host machine
> user will not be on the container

See all the details of the container:
***************

docker inspect containername/id



Delete the container:

$ docker rm -f containerid

Port Mapping to access container from browser
************************
In which user can access your container form browser

in this scenarios, the option used is -p

-p : port forwarding or port mapping

-p systemport:containerport

container port : comes from image

systemport: any free port


after PM/PF

user will requets to system port and requets will be forwarded to container port
applicaiton will displayed on the browser

Note:

Port mapping or forwarding has to be only at runtime, when creating te comtainer

if the container is created in docker, you cannnot do port formwarding

you delete the container and recreate it with port forwarding


$ docker run -d --name n2 -p 8282:80 nginx


$ docker run -d --name n3 -P nginx

Docker Volumes:


Volume mapping & port mapping to be done while creating the comntainer

if container is already created, you cannot mount volume

Volumes are used to preserve data of the container on the local docker host

In this case if the container get deleted in future, the container data will still be available on the host machine in the volume

Volumes are always available under docker area

/var/lib/docker/volumes   ---> docker area

Types of volumes:

 > Named volume

step1: Create Volume and give a name to it

  $ docker volume create myvol

  $ docker volume inspect myvol

 $ docker volume ls

you will find the volume created user

/var/lib/docker/volumes/myvol/_data

Inside the _data directory, containers data will be preserved


Step 2:   mapping (Associating ) volume to the container
*******

for mapping volume the run option is  -v or --volume

on a container multiple volumes can be mapped

 i) preserving data from container to volume

  which directory of container do you want to preserver :  /tmp

   $ docker run -it --name c1 -v myvol:/tmp centos


 container directory whose data will be preserved : /tmp

 where will data be preserved: mount point : /var/lib/docker/volumes/myvol/_data


Verify if really data is available in  volume or not

 $ cd /var/lib/docker/volumes/myvol/_data

data from contaienr will be available


ii) delete the container and check if data is still available or not on the host machine in volume



** 2 : data form volume can be made available on the container  ==> by Volume mapping

** 3: share the data between container  ==> using docker volumes

       1 volume cna be mapped to multiple containers


**   > /etc/conf.xml this file is on the host machine
and  i want it to be available on my container logstash in directory  /var/log

Answer: Using volumes we can make data form host directory to be available on container directpry


> Bind Mounts
*********

  mapping any host directory to container directory

 
 $ docker run -d -v /hostDir:/containerdir imagename

in this case no named volumed volume is created

no _data folder will be there in /var/lib/docker/volumes



End to end scenario for deployment of code on a httpd container

Delete containers:

docker rm -f $(docker ps -aq)

Code repository:

https://github.com/Sonal0409/ecomm.git


make the code to be available on local docker host

$ cd
$ git clone https://github.com/Sonal0409/ecomm.git

$ cd ecomm

Create  a container using image httpd and place code files on the container

Access the container from browser


  $ docker run -d --name myweb -p 8282:80 -v /root/ecomm:/usr/local/apache2/htdocs/ httpd


Creating a docker file and building an image from it
*******************************************************
it is a simple txt file.

# vim dockerfile
	i

FROM centos    // base image information
MAINTAINER edureka
RUN yum -y update            // these are commands that we give for building the image
RUN yum install git -y		// information about packages that shoudl be 					there on the image
VOLUME /data
:wq!

# docker build -t mycentos:git .

When we run the image -->container will be created.

	# docker images


	# docker run --name c1 -it mycentos1:dockerfile
	

==> data folder which is mounted for volume wll be there

	

OTher instructions that we can give in an Image :
FROM: 

MAINTAINER:

RUN:

CMD:

VOLUME:

EXPOSE:

ADD:

**************************************************

Deploy Job in Jenkins:
**************************************
Steps to create a Jenkins job and create docker file and create container for addressbook.war

1. Start the jenkins on AWS instance

# systemctl start jenkins

2. Start docker 
# systemctl start docker

3. Log into jenkins --> go to package job --go to console and copy the path of .war file

4. create a new job -->Deploy

5. Go to build -->shell command

rm -rf mydockerfile
mkdir mydockerfile
cd mydockerfile
cp /var/lib/jenkins/workspace/Package/target/addressbook.war .
touch dockerfile
cat <<EOT>> dockerfile
From tomcat:9
ADD addressbook.war /usr/local/tomcat/webapps
EXPOSE 8080
CMD ["catalina.sh", "run"]
EOT
sudo docker build -t myimage:$BUILD_NUMBER .
sudo docker run -itd -P myimage:$BUILD_NUMBER

Save the job

Go to instance to give jenkins permission to execute docker commands. As of now we are logged in as admin in jenkins and it doesnt have permission to run docker commands.

So go to
vim /etc/sudoers
i
add under root
Jenkins ALL=NOPASSWD: ALL
:wq!

Go back to jenkins and build now.

==> New image and container will be created

# docker images

# docker ps -a

Access from browser

http://18.221.206.57:32768/addressbook/

******************************************************************************

**************************************************




mkdir demoCompose
cd demoCompose

vi docker-compose.yml

version: '2'

services:
  compose-test:
   image: centos:7
   links:
    - compose-db
   command: sleep infinity
  compose-db:
   image: redis
   expose:
    - "6379"

$ docker-compose up -d

$ docker-compose ps

$ docker-compose exec compose-test /bin/bash

[OR]
$ docker-compose exec compose-test sh

# yum install -y nc
# nc compose-db 6379
# ping

# set name Sonal

# get name

Ctrl+C

# exit


$ docker-compose down


Docker Swarm
*****************************

SETUP OF DOCKER SWARM:
******************************
Create 3 AWS instances and name them as Manager, worker 1 and worker 2.

Go to git bash

# cd downloads/

# SSH path from AWS for manager

# sudo su -

# yum install docker

# vim /etc/hostname  ==> to chnage the hostname of this machine to Manager

:wq!  

# init 6  ==> restart the machine


Again open git bash

# cd downloads/

# SSH path from AWS for Worker1

# sudo su -

# yum install docker

# vim /etc/hostname  ==> to chnage the hostname of this machine to Worker1

:wq!  

# init 6  ==> restart the machine

Again open git bash

# cd downloads/

# SSH path from AWS for Worker2


# sudo su -

# yum install docker

# vim /etc/hostname  ==> to chnage the hostname of this machine to Worker2

:wq!  

# init 6  ==> restart the machine

****************************************
Initialize the Swarm Manager:
***************************************
ON MANAGER:
*************
Go to the manager machine and give below command

# systemctl start docker

# docker swarm init --advertise-addr 172.31.17.248(privateip of manager)

Curent node will be now be swarm Manager and it will generate a token id that can be executed on worker nodes so that they will join the swarm as workers.

==> Copy the token id on notepad
************************

WORKER1 SETUP
**********************

# systemctl start docker

paste the token command here from notepad  // as shown below:

# docker swarm join --token SWMTKN-1-66xevqpe5r6h1gtcps1m867q8jcu93deyk87qudqdp7nyf8omk-7vna75w6jip5hmqp4h55bi3s3 172.31.17.248:2377

// the worker1 will now join the swarm as worker
It will give message as: This node joined a swarm as a worker.

MANAGER NODE:

Go to manager node and see if worker1 has joined the swarm or not

# docker node ls

2 nodes will be there .. maanager and worker1


WORKER2 SETUP
**********************

# systemctl start docker

paste the token command here from notepad  // as shown below:

# docker swarm join --token SWMTKN-1-66xevqpe5r6h1gtcps1m867q8jcu93deyk87qudqdp7nyf8omk-7vna75w6jip5hmqp4h55bi3s3 172.31.17.248:2377

// the worker2 will now join the swarm as worker
It will give message as: This node joined a swarm as a worker.

MANAGER NODE:

Go to manager node and see if worker1 has joined the swarm or not

# docker node ls

3 nodes will be there .. maanager and worker1 and worker2

**********SETUP COMPLETE********************************************


SCEANRIO 1 : LOAD BALANCING or distribution of Service

************************************************

Here we will create a service nginx with 5 replicas and perform port mapping

	# docker service create --name mywb -p 8989:80 --replicas 5 nginx

==>This will create 5 replicas of nginx and it is distributed on the nodes.

==> to check the services

	# docker service ps mywb

==> will give information on which on nodes this service is running
==> take public ip of any node on which service is running and open on browser with port 8989

 List all services on SWARM
*******************************
	# docker service ls

*********************************
Delete service on the SWARm

	# docker service rm mywb

//removes service

	# docker service ls  // no service will be there
*****************************************
Scaling
****************************
Scenario2:

Using the concept of scaling we can increase or reduce the replicas


1.Create tomcat service with replica 3 with posrt mapping

# docker service create --name mytomcat  -p 9090:8080 --replicas 3 tomcat

2. 	# docker service ps mytomcat

3. Now scale up the service to 5

	# docker service scale mytomcat=5

4. check if 5 replicas of tomcat are running

	# docker service ps mytomcat    

// total of 5 services will be there 2 more services added.

5. Now scale down the services to 3

	# docker service scale mytomcat=3

6.  check if 3 replicas of tomcat are running

	# docker service ps mytomcat    

// total of 3 services will be there.
Also if service is running on manager and worker1 node,and no service is on worker2 even then we can access the application from worker2 ip address:portnumber , as the posrt mapping is done at cluster level for the service.
all the nodes can access the application irrespective of the service being avialble on it or not.


*****************************************
Create service for custom Image

# docker service create --name mysvc  -p 8181:3000 --replicas 3 lerndevops/samplepyapp:v1

3 replicas will be create dand distributed across the nodes
you can access the app through any ipaddress with same port 8181.. it is service port/cluster wide port

Now lets go to Worker 1 and try this out..

# while true;do curl http://3.23.95.232:8181/;sleep 1; echo " ";done

lets reduce the replicas

# docker service scale mysvc=2

lets increase the replicas

# docker service scale mysvc=4

without any downtime, user requests are being disptibuted to all the containers-- load balancing

Lets do rolling update now

# docker service update --image lerndevops/samplepyapp:v2 mysvc

one by one each replica will be updated without user facing any downtime

roll back

# docker service rollback mysvc


***************************************************************

==> Swarm will always try to maintain number replicas always even if one of the server shuts down.

******************************************
SCENARIO 3: FAILOVER
**************************************
A. Create a httpd service with 6 replicas and delete one replica running on the manager 

1. Create httpd service with 6 replicas

      # docker service create --name mywebserver -p 9090:80 --replicas 6 httpd

2. Check if all 6 replicas are running or not.

 	# docker service ps mywebserver

3. On the MANAGER machine, delete a replica

	# docker ps -a  // replicas will be displayed
	# docker rm -f containername  // replica will be deleted
	
# docker ps -a
# docker service ps mywebserver

now replica from Mnager will be with desired state as Shutdown and a replica will be running on worker 1 or worker2
In any case the swarm will always maintain the total count of replicas as 4


3. Drain worker1 from the swarm 
	
	# docker node ls
	# docker node update --availability drain Worker1

4. check if all 6 replicas are running on manager and worker2
 
	# docker service ps mywebserver
	
	# docker service ps mywebserver | grep Running

5. Make worker1 join the swarm again

        # docker node update --availability active Worker1

6. check if all 6 replicas are running on manager and worker1 and 2
 
	# docker service ps mywebserver

7. we can also go to worker2 git bash and leave the swarm
	# systemctl start docker
	# docker swarm leave


8.Connect back to manager

# docker node ls

worker 2 will be in status down and Active

9. Remove the worker2 node from the swarm

	# docker node rm Worker2

Worker 2 will not be in swarm now.

	# docker node ls

9.Check if all the replicas are still running or not.

	# docker services ps webserver

10. join back the worker2 to swarm

	# docker swarm join-token worker

copy the token and paste it on worker 2 machine

docker swarm join --token SWMTKN-1-195rkd1a6332n2izpccyfn9d29q1mqs72ja0d4ueo63xyetrum-6t7d8m9tfcw9qtj86ske56p0q 172.31.44.159:2377

Worker2 will join the swarm again

on Manager machine:  # docker node ls   // worker 2 will be active and running

**********************************************************
ROLLING UPDATE/ ROLLING OUT:
*************************************************************

Create redis:3 with 5 replicas and update it to redis:4 and latter roll it back to redis:3

1.Create redis:3 with 5 replicas

# docker service create --name myredis --replicas 5 redis:3

2.Check if 5 replicas are running or not

Docker service ps myredis

3.perform a rolling update from redis:3 to redis:4

# docker service update --image redis:4 myredis

4.Check if 5 replicas of redis:3 are shutdown and redis:4 are up and running

# docker service ps myredis

5.Perform roll back from redis:4 to redis:3

# docker service update --rollback myredis

6.Check if 5 replicas of redis:4 are shutdown and redis:3 are up and running

# docker service ps myredis

*****************************************************












































































































